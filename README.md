# ğŸª Data Pipeline Retail

Projet personnel de data engineering simulant un pipeline de donnÃ©es retail en environnement moderne (ELT) avec ingestion, transformation, tests et orchestration.

---

## ğŸ“Œ Objectifs pÃ©dagogiques

- Comprendre et implÃ©menter un pipeline ELT complet
- Utiliser les outils modernes de la data stack : Airflow, DBT, SQL, Python
- Appliquer les bonnes pratiques de structuration, tests et documentation
- Simuler une stack cloud (Snowflake, etc.) en environnement local

---

## âš™ï¸ Outils utilisÃ©s

| Outil        | RÃ´le                                               |
|--------------|----------------------------------------------------|
| Python       | Scripts d'ingestion et manipulation de donnÃ©es     |
| Pandas       | Nettoyage et transformation initiale               |
| SQLite / PostgreSQL | Stockage local simulant Snowflake           |
| SQLAlchemy   | Connexion et insertion dans la base                |
| DBT          | Transformations SQL + tests + documentation        |
| Airflow      | Orchestration du pipeline (DAG)                    |
| GitHub       | Versionnage et publication                         |

---

## ğŸ“ Structure du projet

data-pipeline-retail/ â”œâ”€â”€ airflow/ â† DAG Airflow â”œâ”€â”€ dbt/ â† Projet DBT (modÃ¨les SQL, testsâ€¦) â”œâ”€â”€ data/ â† DonnÃ©es brutes (CSV) â”œâ”€â”€ notebooks/ â† Analyses exploratoires ou KPIs â”œâ”€â”€ etl/ â† Scripts Python (extraction, chargement) â”œâ”€â”€ README.md â”œâ”€â”€ requirements.txt â””â”€â”€ docker-compose.yml â† (optionnel) Setup local


---

## ğŸš€ Ã€ venir

- Ajout dâ€™un DAG Airflow pour orchestrer le pipeline
- CrÃ©ation de modÃ¨les DBT : staging, marts, KPIs
- Documentation automatique avec DBT docs
- Test de qualitÃ© sur les donnÃ©es

---

## ğŸ“¸ Captures (Ã  venir)

- AperÃ§u du DAG Airflow
- RÃ©sultats de tests DBT
- Extraits de donnÃ©es transformÃ©es
